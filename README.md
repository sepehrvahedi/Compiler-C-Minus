# C-Minus Compiler Project

## Project Goal

The overall goal of this project is to design and implement a multi-phase compiler for the C-minus programming language, a simplified subset of the C language. The compiler will translate C-minus source code into a target representation (often assembly or machine code, though the specific target might vary depending on project requirements). This involves several distinct phases:

1.  **Lexical Analysis (Scanner):** Reads the source code and breaks it down into a stream of tokens (Phase 1 - Completed).
2.  **Syntax Analysis (Parser):** Analyzes the token stream to check if it conforms to the grammatical rules of the C-minus language, typically building a parse tree or abstract syntax tree (AST).
3.  **Semantic Analysis:** Checks the AST for semantic correctness (e.g., type checking, variable declarations).
4.  **Intermediate Code Generation (Optional):** Translates the AST into a machine-independent intermediate representation.
5.  **Code Optimization (Optional):** Improves the intermediate code for efficiency.
6.  **Target Code Generation:** Translates the (optimized) intermediate code into the final target language (e.g., assembly).

This project provides practical experience in compiler construction principles and techniques.

## Phase 1: Lexical Analysis (Scanner)

This first phase focuses on **Lexical Analysis**, often called **Scanning** or **Tokenization**.

**What We Did in Phase 1:**

*   **Implemented a Scanner:** We created a `Scanner` class (`scanner.py`) responsible for reading the input C-minus source code character by character from `input.txt`.
*   **Token Recognition:** The scanner identifies and categorizes sequences of characters into meaningful units called **tokens**. These include:
    *   **Keywords:** Reserved words like `if`, `else`, `int`, `void`, `return`, `while`.
    *   **Identifiers (ID):** Names for variables and functions (e.g., `a`, `main`, `cde`).
    *   **Numbers (NUM):** Integer literals (e.g., `0`, `2`, `3`).
    *   **Symbols:** Operators and punctuation (e.g., `+`, `-`, `=`, `==`, `<`, `(`, `)`, `{`, `}`, `;`, `,`).
*   **Whitespace and Comments:** The scanner correctly identifies and skips over whitespace (spaces, tabs, newlines) and comments (`/* ... */`). These are generally ignored by later phases but must be handled correctly here.
*   **Error Handling:** Implemented basic lexical error detection. If the scanner encounters a sequence of characters that doesn't form a valid token (e.g., `3d`, `@2`, an unterminated comment `*/`), it records an error message along with the line number and the invalid character sequence. It uses a simple "panic mode" to skip characters until it finds something that can potentially start a new token, allowing it to continue scanning the rest of the file.
*   **Symbol Table:** Created a basic symbol table (`symbol_table.txt`) which stores all recognized keywords and identifiers. Keywords are pre-loaded, and identifiers are added as they are encountered in the source code.
*   **Driver Program:** Developed the main driver script (`compiler.py`) that:
    *   Reads the source code from `input.txt`.
    *   Instantiates and runs the `Scanner`.
    *   Collects the generated tokens, errors, and symbol table entries.
    *   Writes the results into formatted output files (`tokens.txt`, `lexical_errors.txt`, `symbol_table.txt`).
*   **Token Definitions:** Defined token types constants in `token_types.py` for better code readability and maintenance.

## Project Structure (Phase 1)

```
.
├── compiler.py         # Main driver script for the scanner phase
├── scanner.py          # Contains the Scanner class implementation
├── token_types.py      # Defines token type constants and keyword/symbol lists
├── input.txt           # Input C-minus source code file
│
├── tokens.txt          # Output: Recognized tokens (generated by compiler.py)
├── lexical_errors.txt  # Output: Detected lexical errors (generated by compiler.py)
└── symbol_table.txt    # Output: Keywords and Identifiers (generated by compiler.py)
```

## Requirements

*   **Python 3.8+** (The code uses features like f-strings and type hinting which are standard in recent Python 3 versions).

## How to Use (Run Phase 1)

1.  **Ensure Files:** Make sure the files `compiler.py`, `scanner.py`, `token_types.py`, and `input.txt` are all present in the same directory.
2.  **Prepare Input:** Place the C-minus source code you want to analyze into the `input.txt` file. You can modify the provided `input.txt` or replace its content.
3.  **Open Terminal:** Navigate to the directory containing the files using your terminal or command prompt.
4.  **Run:** Execute the main driver script using Python:
    ```bash
    python3 compiler.py
    ```
    (or `python compiler.py` if `python` on your system points to Python 3.8+).

5.  **Check Outputs:** The script will run the scanner on `input.txt` and generate/overwrite three files in the same directory: `tokens.txt`, `lexical_errors.txt`, and `symbol_table.txt`.

## Output Files Explained

The execution of `python3 compiler.py` generates the following files:

1.  **`tokens.txt`**
    *   **Purpose:** Lists all the valid tokens recognized by the scanner from `input.txt`, grouped by the line number on which they appear. Comments and whitespace are *not* included.
    *   **Format:** Each line corresponds to a line in the source file that contained tokens.
        ```
        {lineno}<TAB>(TOKEN_TYPE, lexeme) (TOKEN_TYPE, lexeme) ...
        ```
        *   `{lineno}`: The line number from `input.txt`.
        *   `<TAB>`: A literal tab character separates the line number from the tokens.
        *   `(TOKEN_TYPE, lexeme)`: A pair representing a single token, where `TOKEN_TYPE` is the category (e.g., `KEYWORD`, `ID`, `NUM`, `SYMBOL`) and `lexeme` is the actual text sequence from the input (e.g., `if`, `a`, `10`, `+`). Tokens on the same line are separated by spaces.

2.  **`lexical_errors.txt`**
    *   **Purpose:** Reports any lexical errors encountered during scanning.
    *   **Format:**
        *   If no errors are found, the file will contain the single line:
            ```
            There is no lexical error.
            ```
        *   If errors are found, each line represents one error:
            ```
            {lineno}<TAB>({invalid_sequence}, {error_message})
            ```
            *   `{lineno}`: The line number where the error occurred.
            *   `<TAB>`: A literal tab character.
            *   `{invalid_sequence}`: The specific text from the input that caused the error.
            *   `{error_message}`: A brief description of the error (e.g., `Invalid input`, `Unmatched comment`). Errors are listed in the order they appear in the source code (sorted by line number).

3.  **`symbol_table.txt`**
    *   **Purpose:** Lists all unique keywords and identifiers found in the input code. This table is crucial for later phases (like semantic analysis) to keep track of variables and functions.
    *   **Format:** Each line represents one entry in the table, numbered sequentially starting from 1.
        ```
        {index}.<TAB>{lexeme}
        ```
        *   `{index}`: A sequential number (1, 2, 3, ...).
        *   `<TAB>`: A literal tab character.
        *   `{lexeme}`: The keyword or identifier string.
    *   **Order:** All C-minus keywords (`else`, `if`, `int`, `return`, `void`, `while`) appear first in the table, followed by the identifiers encountered in the source code, listed in the order of their *first* appearance.

## Future Phases

The output of this phase (`tokens.txt` primarily, along with the symbol table concepts) serves as the input for the next phase of the compiler: **Syntax Analysis (Parsing)**. The parser will consume the token stream to verify the grammatical structure of the C-minus program.